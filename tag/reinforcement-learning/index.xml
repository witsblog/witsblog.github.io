<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning | Wit's Blog</title><link>https://witsblog.github.io/tag/reinforcement-learning/</link><atom:link href="https://witsblog.github.io/tag/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><description>Reinforcement Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Aug 2018 00:00:00 +0000</lastBuildDate><image><url>https://witsblog.github.io/media/icon_hu66cf44b0d2eab4dae9de00f68d847da7_1567_512x512_fill_lanczos_center_3.png</url><title>Reinforcement Learning</title><link>https://witsblog.github.io/tag/reinforcement-learning/</link></image><item><title>Autonomous Ball-Collecting Robot using Reinforcement Learning</title><link>https://witsblog.github.io/project/00_robot/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://witsblog.github.io/project/00_robot/</guid><description>&lt;p>This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor. The algorithms were implemented on &lt;a href="https://www.intel.com/content/www/us/en/products/details/nuc.html" target="_blank" rel="noopener">Intel NUC&lt;/a> and &lt;a href="https://developer.nvidia.com/embedded/jetson-tx2" target="_blank" rel="noopener">Nvidia Jetson TX2&lt;/a> board.&lt;/p>
&lt;p>This is my attempt at re-writing a write-up because a lot of recruiters ask me how the project was done and I sometimes forget (it was done in 2018).&lt;/p>
&lt;br>
&lt;h2 id="optimal-policy-and-optimal-value-functions">Optimal Policy and Optimal Value Functions&lt;/h2>
&lt;p>&lt;strong>Bellman optimality equation&lt;/strong>&lt;/p>
&lt;p>For optimal value function
$v_{*}$:&lt;/p>
$$
\begin{aligned}
v_{*}(s) &amp;= \max_{a} E \left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \vert S_{t}=s, A_{t}=a \right] \\
&amp;= \max_{a} \sum_{s',r} p(s',r \vert s,a) \left[ r + \gamma v_{*}(s') \right]
\end{aligned}
$$
&lt;p>For optimal action-value function
$q_{*}$:&lt;/p>
$$
\begin{aligned}
q_{*}(s,a) &amp;= E \left[ R_{t+1} + \gamma \max_{a'} q_{*}(S_{t+1}, a') \vert S_{t} = s, A_{t} = a \right] \\
&amp;= \sum_{s',r} p(s',r \vert s,a) \left[ r + \gamma \max_{a'} q_{*}(s',a') \right]
\end{aligned}
$$
&lt;h2 id="monte-carlo-methods">Monte Carlo Methods&lt;/h2>
&lt;p>Monte Carlo methods only require a sample of states, actions, and rewards from interaction between the agent and the environment. It is model-free; the probability distributions such as state-transition
$p(s' \vert s,a)$ need not to be known.&lt;/p>
&lt;h2 id="q-learning">Q-Learning&lt;/h2>
$$ Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') \right] $$
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Initialize Q(s, a)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Start with state &amp;#34;s&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Loop:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Select action &amp;#34;a&amp;#34; with e-greedy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Execute action &amp;#34;a&amp;#34;, receive immediate reward &amp;#34;r&amp;#34; and go to state &amp;#34;s&amp;#39;&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Q(s, a) = Q(s, a) + alpha*[r + gamma * max{Q(s&amp;#39;, a&amp;#39;)} - Q(s, a)]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But what if the number of
$(s,a)$ pairs are very big? Then it is not practical to keep the table
$Q$ for every pair of
$(s,a)$. Instead, we could maintain
$Q(s,a)$ as a parameterized function. This is where the neural network comes into play.&lt;/p>
&lt;h2 id="q-network">Q-Network&lt;/h2>
&lt;p>&lt;strong>Training&lt;/strong>&lt;/p>
&lt;p>We define the &lt;strong>loss&lt;/strong> as:&lt;/p>
$$
L(\theta) = \left( \left( r + \gamma \max_{a'} {Q(s', a' | \theta }) \right) - Q(s,a|\theta) \right)^2
$$
&lt;p>Our objective is to find weight $\theta$ of the network to minimize the loss:&lt;/p>
$$
\min_{\theta} L(\theta) = \min_{\theta} \left[ \left( r + \gamma \max_{a'} {Q(s', a' | \theta }) - Q(s,a|\theta) \right)^2 \right]
$$
&lt;p>where
$r$ is the immediate reward that we observe and the term
$r + \gamma \max_{a'} {Q(s', a' | \theta })$ is the approximated target.&lt;/p>
&lt;p>&lt;strong>Convergence&lt;/strong>&lt;/p>
&lt;p>However, using a neural network to represent the action-value function tends to be unstable due to:&lt;/p>
&lt;ul>
&lt;li>Correlations between samples&lt;/li>
&lt;li>Non-stationary targets&lt;/li>
&lt;/ul>
&lt;p>How does DeepMind solve this issue?&lt;/p>
&lt;ul>
&lt;li>Experience replay&lt;/li>
&lt;li>Separated networks&lt;/li>
&lt;li>Go deeper (added by myself)&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://witsblog.github.io/images/project/00/go_deeper.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="experience-replay">Experience Replay&lt;/h2>
&lt;h2 id="separated-networks">Separated Networks&lt;/h2>
$$
L(\theta) = \left( \left( r + \gamma \max_{a'} {Q(s', a' | \theta^{-} }) \right) - Q(s,a|\theta) \right)^2
$$
&lt;br>
&lt;h2 id="gallery">Gallery&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://witsblog.github.io/images/project/00/1.jpeg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://witsblog.github.io/images/project/00/2.jpeg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://witsblog.github.io/images/project/00/3.jpeg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://witsblog.github.io/images/project/00/4.jpeg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>Mnih, V. et al. &lt;a href="https://deepmind.com/research/publications/2019/human-level-control-through-deep-reinforcement-learning" target="_blank" rel="noopener">Human-level control through deep reinforcement learning.&lt;/a> Nature 518, 529â€“533 (2015).&lt;/li>
&lt;/ul></description></item></channel></rss>