<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: March 6, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Fira+Code&family=Lora&family=Work+Sans&family=Montserrat:wght@400;700&family=Roboto+Mono:wght@400;700&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Fira+Code&family=Lora&family=Work+Sans&family=Montserrat:wght@400;700&family=Roboto+Mono:wght@400;700&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.176e9a6c2874fb96332dfb10e8718f70.css><link rel=stylesheet href=/css/libs/chroma/github-dark.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/github-dark.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Wit Sirawit"><meta name=description content="This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor."><link rel=alternate hreflang=en-us href=https://witsblog.github.io/project/00_robot/><link rel=canonical href=https://witsblog.github.io/project/00_robot/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu66cf44b0d2eab4dae9de00f68d847da7_1567_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu66cf44b0d2eab4dae9de00f68d847da7_1567_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#2962ff"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://witsblog.github.io/media/icon_hu66cf44b0d2eab4dae9de00f68d847da7_1567_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Wit's Blog"><meta property="og:url" content="https://witsblog.github.io/project/00_robot/"><meta property="og:title" content="Autonomous Ball-Collecting Robot using Reinforcement Learning | Wit's Blog"><meta property="og:description" content="This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor."><meta property="og:image" content="https://witsblog.github.io/media/icon_hu66cf44b0d2eab4dae9de00f68d847da7_1567_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2018-08-01T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://witsblog.github.io/project/00_robot/"},"headline":"Autonomous Ball-Collecting Robot using Reinforcement Learning","datePublished":"2018-08-01T00:00:00Z","dateModified":"2018-08-01T00:00:00Z","author":{"@type":"Person","name":"Wit Sirawit"},"publisher":{"@type":"Organization","name":"Wit's Blog","logo":{"@type":"ImageObject","url":"https://witsblog.github.io/media/icon_hu66cf44b0d2eab4dae9de00f68d847da7_1567_192x192_fill_lanczos_center_3.png"}},"description":"This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor."}</script><title>Autonomous Ball-Collecting Robot using Reinforcement Learning | Wit's Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3c1241fed2e2d76a3623317d0c5bd7b8><script src=/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Wit's Blog</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Wit's Blog</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#about><span>About</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#gallery><span>Gallery</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><article class="article article-project"><div class="article-container pt-3"><h1>Autonomous Ball-Collecting Robot using Reinforcement Learning</h1><div class=article-metadata><span class=article-date>Aug 1, 2018</span></div></div><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#optimal-policy-and-optimal-value-functions>Optimal Policy and Optimal Value Functions</a></li><li><a href=#monte-carlo-methods>Monte Carlo Methods</a></li><li><a href=#q-learning>Q-Learning</a></li><li><a href=#q-network>Q-Network</a></li><li><a href=#experience-replay>Experience Replay</a></li><li><a href=#separated-networks>Separated Networks</a></li><li><a href=#gallery>Gallery</a></li><li><a href=#references>References</a></li></ul></nav></div><div class=article-container><div class=article-style><p>This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor. The algorithms were implemented on <a href=https://www.intel.com/content/www/us/en/products/details/nuc.html target=_blank rel=noopener>Intel NUC</a> and <a href=https://developer.nvidia.com/embedded/jetson-tx2 target=_blank rel=noopener>Nvidia Jetson TX2</a> board.</p><p>This is my attempt at re-writing a write-up because a lot of recruiters ask me how the project was done and I sometimes forget (it was done in 2018).</p><br><h2 id=optimal-policy-and-optimal-value-functions>Optimal Policy and Optimal Value Functions</h2><p><strong>Bellman optimality equation</strong></p><p>For optimal value function
$v_{*}$:</p>$$
\begin{aligned}
v_{*}(s) &= \max_{a} E \left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \vert S_{t}=s, A_{t}=a \right] \\
&= \max_{a} \sum_{s',r} p(s',r \vert s,a) \left[ r + \gamma v_{*}(s') \right]
\end{aligned}
$$<p>For optimal action-value function
$q_{*}$:</p>$$
\begin{aligned}
q_{*}(s,a) &= E \left[ R_{t+1} + \gamma \max_{a'} q_{*}(S_{t+1}, a') \vert S_{t} = s, A_{t} = a \right] \\
&= \sum_{s',r} p(s',r \vert s,a) \left[ r + \gamma \max_{a'} q_{*}(s',a') \right]
\end{aligned}
$$<h2 id=monte-carlo-methods>Monte Carlo Methods</h2><p>Monte Carlo methods only require a sample of states, actions, and rewards from interaction between the agent and the environment. It is model-free; the probability distributions such as state-transition
$p(s' \vert s,a)$ need not to be known.</p><h2 id=q-learning>Q-Learning</h2>$$ Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') \right] $$<div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Initialize Q(s, a)
</span></span><span class=line><span class=cl>Start with state &#34;s&#34;
</span></span><span class=line><span class=cl>Loop:
</span></span><span class=line><span class=cl>    Select action &#34;a&#34; with e-greedy
</span></span><span class=line><span class=cl>    Execute action &#34;a&#34;, receive immediate reward &#34;r&#34; and go to state &#34;s&#39;&#34;
</span></span><span class=line><span class=cl>    Q(s, a) = Q(s, a) + alpha*[r + gamma * max{Q(s&#39;, a&#39;)} - Q(s, a)]
</span></span></code></pre></div><p>But what if the number of
$(s,a)$ pairs are very big? Then it is not practical to keep the table
$Q$ for every pair of
$(s,a)$. Instead, we could maintain
$Q(s,a)$ as a parameterized function. This is where the neural network comes into play.</p><h2 id=q-network>Q-Network</h2><p><strong>Training</strong></p><p>We define the <strong>loss</strong> as:</p>$$
L(\theta) = \left( \left( r + \gamma \max_{a'} {Q(s', a' | \theta }) \right) - Q(s,a|\theta) \right)^2
$$<p>Our objective is to find weight $\theta$ of the network to minimize the loss:</p>$$
\min_{\theta} L(\theta) = \min_{\theta} \left[ \left( r + \gamma \max_{a'} {Q(s', a' | \theta }) - Q(s,a|\theta) \right)^2 \right]
$$<p>where
$r$ is the immediate reward that we observe and the term
$r + \gamma \max_{a'} {Q(s', a' | \theta })$ is the approximated target.</p><p><strong>Convergence</strong></p><p>However, using a neural network to represent the action-value function tends to be unstable due to:</p><ul><li>Correlations between samples</li><li>Non-stationary targets</li></ul><p>How does DeepMind solve this issue?</p><ul><li>Experience replay</li><li>Separated networks</li><li>Go deeper (added by myself)</li></ul><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/project/00/go_deeper.jpg alt loading=lazy data-zoomable></div></div></figure></p><h2 id=experience-replay>Experience Replay</h2><h2 id=separated-networks>Separated Networks</h2>$$
L(\theta) = \left( \left( r + \gamma \max_{a'} {Q(s', a' | \theta^{-} }) \right) - Q(s,a|\theta) \right)^2
$$<br><h2 id=gallery>Gallery</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/project/00/1.jpeg alt loading=lazy data-zoomable></div></div></figure></p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/project/00/2.jpeg alt loading=lazy data-zoomable></div></div></figure></p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/project/00/3.jpeg alt loading=lazy data-zoomable></div></div></figure></p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/project/00/4.jpeg alt loading=lazy data-zoomable></div></div></figure></p><h2 id=references>References</h2><ul><li>Mnih, V. et al. <a href=https://deepmind.com/research/publications/2019/human-level-control-through-deep-reinforcement-learning target=_blank rel=noopener>Human-level control through deep reinforcement learning.</a> Nature 518, 529–533 (2015).</li></ul></div><div class=article-tags><a class="badge badge-light" href=/tag/robotics/>Robotics</a>
<a class="badge badge-light" href=/tag/reinforcement-learning/>Reinforcement Learning</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwitsblog.github.io%2Fproject%2F00_robot%2F&amp;text=Autonomous+Ball-Collecting+Robot+using+Reinforcement+Learning" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fwitsblog.github.io%2Fproject%2F00_robot%2F&amp;t=Autonomous+Ball-Collecting+Robot+using+Reinforcement+Learning" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fwitsblog.github.io%2Fproject%2F00_robot%2F&amp;title=Autonomous+Ball-Collecting+Robot+using+Reinforcement+Learning" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li></ul></div><div class="project-related-pages content-widget-hr"></div></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer></footer></div></div><script src=/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.2945731fa88332cb0ec33cf47d29879f.js></script></body></html>